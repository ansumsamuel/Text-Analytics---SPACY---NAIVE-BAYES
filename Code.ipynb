{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>a24415.txt</td>\n",
       "      <td>Eat alot!  Food is good for the soul.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>ans1373.txt</td>\n",
       "      <td>The level of severity depend on the amount of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>a24359.txt</td>\n",
       "      <td>Thats a good thing.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>a31572.txt</td>\n",
       "      <td>Allergies can get worse over time but there ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>ans439.txt</td>\n",
       "      <td>I do understand your concern for cervical canc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>ans49.txt</td>\n",
       "      <td>According to your description it seems that yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>a54316.txt</td>\n",
       "      <td>hamburgers and french fries work for me</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>a61231.txt</td>\n",
       "      <td>you are lucky to have not had the decisions ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>a54319.txt</td>\n",
       "      <td>well first of all  make sure its a cold and  n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>a69453.txt</td>\n",
       "      <td>You can always be a Monk.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>ans1834.txt</td>\n",
       "      <td>Your symptoms and test results are probably in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>ans1843.txt</td>\n",
       "      <td>Your symptoms do suggest of a pulmonary infect...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>ans1763.txt</td>\n",
       "      <td>You are most probably suffering from seborrhei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>a7535.txt</td>\n",
       "      <td>you could be pregnant, your iron levels in you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>a31606.txt</td>\n",
       "      <td>ive never tried it. But i read experimention, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>a24392.txt</td>\n",
       "      <td>FIRST OFF WHO ARE YOU?  YOU ARE A CHILD OF THE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>a31679.txt</td>\n",
       "      <td>I cringe just thinking of that.  OW!!!!  If yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>a67.txt</td>\n",
       "      <td>Having had a deep cleaning done recently, I ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file                                               text  class\n",
       "183    a24415.txt             Eat alot!  Food is good for the soul.       0\n",
       "416   ans1373.txt  The level of severity depend on the amount of ...      1\n",
       "127    a24359.txt                               Thats a good thing.       0\n",
       "648    a31572.txt  Allergies can get worse over time but there ar...      0\n",
       "1252   ans439.txt  I do understand your concern for cervical canc...      1\n",
       "1308    ans49.txt  According to your description it seems that yo...      1\n",
       "910    a54316.txt           hamburgers and french fries work for me       0\n",
       "934    a61231.txt  you are lucky to have not had the decisions ca...      0\n",
       "913    a54319.txt  well first of all  make sure its a cold and  n...      0\n",
       "1321   a69453.txt                         You can always be a Monk.       0\n",
       "928   ans1834.txt  Your symptoms and test results are probably in...      1\n",
       "938   ans1843.txt  Your symptoms do suggest of a pulmonary infect...      1\n",
       "849   ans1763.txt  You are most probably suffering from seborrhei...      1\n",
       "1749    a7535.txt  you could be pregnant, your iron levels in you...      0\n",
       "682    a31606.txt  ive never tried it. But i read experimention, ...      0\n",
       "160    a24392.txt  FIRST OFF WHO ARE YOU?  YOU ARE A CHILD OF THE...      0\n",
       "755    a31679.txt  I cringe just thinking of that.  OW!!!!  If yo...      0\n",
       "1260      a67.txt  Having had a deep cleaning done recently, I ch...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) read the data into a pandas dataframe\n",
    "import os\n",
    "def data2df (path, label):\n",
    "    file, text = [], []\n",
    "    for f in os.listdir(path):\n",
    "        file.append(f)\n",
    "        fhr = open(path+f, 'r', encoding='utf-8', errors='ignore') \n",
    "        t = fhr.read()\n",
    "        text.append(t)\n",
    "        fhr.close()\n",
    "    return(pd.DataFrame({'file': file, 'text': text, 'class':label}))\n",
    "\n",
    "dfneg = data2df('HealthProNonPro/NonPro/', 0) # NonPro\n",
    "dfpos = data2df('HealthProNonPro/Pro/', 1) # Pro\n",
    "\n",
    "df = pd.concat([dfpos, dfneg], axis=0)\n",
    "df.sample(frac=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439     The most common cause of itchy palms is contac...\n",
       "720     1. It takes two to make the money to support t...\n",
       "307     there isnt always signs\\nso do not do it until...\n",
       "87      Speed is an amphetamine, a psychostimulant, wh...\n",
       "1066                                         tantric sex \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2)\tSetup the data for Training/Testing. Use 20% for testing.\n",
    "X, y = df['text'], df['class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "Xtrain = Xtrain.copy()\n",
    "Xtest = Xtest.copy()\n",
    "ytrain = ytrain.copy()\n",
    "ytest = ytest.copy()\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing using Spacy\n",
    "def custom_tokenizer(doc):\n",
    "\n",
    "    # use spacy to filter out noise\n",
    "    tokens = [token.lemma_.lower() \n",
    "                        for token in doc \n",
    "                               if (\n",
    "                                    len(token) >= 2 and # only preserve tokens that are greater than 2 characters long\n",
    "                                    #token.pos_ in ['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV'] and # only preserve selected pos\n",
    "                                    #token.text in nlp.vocab and # check if token in vocab \n",
    "                                    token.is_alpha and # only preserve tokens that are fully alpha (not numeric or alpha-numeric)\n",
    "                                    #not token.is_digit and # get rid of tokens that are fully numeric\n",
    "                                    not token.is_punct and # get rid of tokens that are punctuations\n",
    "                                    not token.is_space and # get rid of tokens that are spaces\n",
    "                                    not token.is_stop and # get rid of tokens that are stop words\n",
    "                                    not token.is_currency # get rid of tokens that denote currencies\n",
    "                                )\n",
    "                   ]\n",
    "\n",
    "    # return cleaned-up text\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439     common cause itchy palm contact dermatitis exp...\n",
       "720     take money support famlys need like food shelt...\n",
       "307                      not sign married doctor approval\n",
       "87      speed amphetamine psychostimulant commonly abu...\n",
       "1066                                          tantric sex\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing Xtrain using Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])\n",
    "corpus = nlp.pipe(list(Xtrain))\n",
    "clean_corpus = [custom_tokenizer(doc) for doc in corpus]\n",
    "Xtrain = pd.Series(clean_corpus,index=Xtrain.index)\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4)Setup a Pipeline with TfidfVectorizer and Naïve Bayes. \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "nb=Pipeline(steps=[('tfidf',TfidfVectorizer()),('mnb',MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Grid Search with 4-fold Cross Validation to search for the best values \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'mnb__alpha': [1.0,0.5,0.25,1.5], # getting best alpha\n",
    "    'tfidf__sublinear_tf':[True,False], # fublinear_tf from tfidf\n",
    "    'tfidf__norm':['l1','l2'] #finding best norm\n",
    "}\n",
    "gscv = GridSearchCV(nb, param_grid, cv=4, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ue,\n",
      "        vocabulary=None)), ('mnb', MultinomialNB(alpha=0.25, class_prior=None, fit_prior=True))]) \n",
      "\n",
      "--------------------------------------------------\n",
      "0.9323770491803278 \n",
      "\n",
      "--------------------------------------------------\n",
      "{'mnb__alpha': 0.25, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False} \n",
      "\n",
      "--------------------------------------------------\n",
      "{'mean_fit_time': array([0.13672966, 0.11328357, 0.11928457, 0.11719626, 0.12500364,\n",
      "       0.11327982, 0.13098782, 0.10937279, 0.11719352, 0.16797191,\n",
      "       0.17969096, 0.13672692, 0.14453399, 0.13672298, 0.14844042,\n",
      "       0.13281971]), 'std_fit_time': array([0.0067735 , 0.01702325, 0.02040568, 0.01353168, 0.02209675,\n",
      "       0.01702815, 0.01693143, 0.01105639, 0.01746941, 0.05108078,\n",
      "       0.01353227, 0.02310619, 0.01295445, 0.02311085, 0.02343684,\n",
      "       0.00780672]), 'mean_score_time': array([0.03905314, 0.03125   , 0.03515333, 0.03124899, 0.03514898,\n",
      "       0.03125358, 0.03905779, 0.02734733, 0.04296494, 0.05468655,\n",
      "       0.05863464, 0.03515029, 0.04296714, 0.03124887, 0.05078083,\n",
      "       0.03905672]), 'std_score_time': array([7.80147356e-03, 2.08531090e-06, 6.76379320e-03, 3.83837252e-06,\n",
      "       6.76975257e-03, 8.92239924e-06, 7.81148700e-03, 6.76927031e-03,\n",
      "       6.76541103e-03, 2.34365861e-02, 6.78838156e-03, 6.76789266e-03,\n",
      "       6.76654614e-03, 2.43942522e-06, 1.29535350e-02, 7.81935880e-03]), 'param_mnb__alpha': masked_array(data=[1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 0.25, 0.25,\n",
      "                   0.25, 0.25, 1.5, 1.5, 1.5, 1.5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_tfidf__norm': masked_array(data=['l1', 'l1', 'l2', 'l2', 'l1', 'l1', 'l2', 'l2', 'l1',\n",
      "                   'l1', 'l2', 'l2', 'l1', 'l1', 'l2', 'l2'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_tfidf__sublinear_tf': masked_array(data=[True, False, True, False, True, False, True, False,\n",
      "                   True, False, True, False, True, False, True, False],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'mnb__alpha': 1.0, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 1.0, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 1.0, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 1.0, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.5, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.5, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.5, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.5, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.25, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.25, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 0.25, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 0.25, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 1.5, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 1.5, 'tfidf__norm': 'l1', 'tfidf__sublinear_tf': False}, {'mnb__alpha': 1.5, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True}, {'mnb__alpha': 1.5, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False}], 'split0_test_score': array([0.91678035, 0.91814461, 0.91950887, 0.92223738, 0.92360164,\n",
      "       0.92223738, 0.92087312, 0.92496589, 0.92633015, 0.92905866,\n",
      "       0.93042292, 0.93178718, 0.90995907, 0.90723056, 0.91405184,\n",
      "       0.91405184]), 'split1_test_score': array([0.93306011, 0.93169399, 0.93989071, 0.94125683, 0.93989071,\n",
      "       0.93989071, 0.94262295, 0.94672131, 0.93989071, 0.93989071,\n",
      "       0.94262295, 0.93989071, 0.93032787, 0.93032787, 0.93579235,\n",
      "       0.93442623]), 'split2_test_score': array([0.89617486, 0.89617486, 0.91256831, 0.91120219, 0.90027322,\n",
      "       0.90300546, 0.91530055, 0.91530055, 0.90846995, 0.90710383,\n",
      "       0.91393443, 0.91803279, 0.89617486, 0.89344262, 0.89754098,\n",
      "       0.90437158]), 'split3_test_score': array([0.92339261, 0.91928865, 0.92886457, 0.93023256, 0.92202462,\n",
      "       0.92339261, 0.93570451, 0.93433653, 0.92749658, 0.92749658,\n",
      "       0.93980848, 0.93980848, 0.91381669, 0.91655267, 0.92065663,\n",
      "       0.92339261]), 'mean_test_score': array([0.91734973, 0.91632514, 0.92520492, 0.92622951, 0.92144809,\n",
      "       0.92213115, 0.92862022, 0.93032787, 0.92554645, 0.92588798,\n",
      "       0.93169399, 0.93237705, 0.91256831, 0.91188525, 0.9170082 ,\n",
      "       0.91905738]), 'std_test_score': array([0.01352758, 0.01278984, 0.01026183, 0.01099548, 0.0140844 ,\n",
      "       0.01306517, 0.01099669, 0.01161252, 0.0112002 , 0.01184875,\n",
      "       0.0112051 , 0.00891224, 0.01217087, 0.01345094, 0.01372827,\n",
      "       0.01113252]), 'rank_test_score': array([12, 14,  8,  5, 10,  9,  4,  3,  7,  6,  2,  1, 15, 16, 13, 11])} \n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 6) Use the Best Estimator resulting from the Grid Search for fitting training dataset and finding best-estimator\n",
    "\n",
    "gscv.fit(Xtrain, ytrain)\n",
    "\n",
    "print (\"-\"*50)\n",
    "print(gscv.best_estimator_, \"\\n\")\n",
    "print (\"-\"*50)\n",
    "print(gscv.best_score_, \"\\n\")\n",
    "print (\"-\"*50)\n",
    "print(gscv.best_params_, \"\\n\")\n",
    "print (\"-\"*50)\n",
    "print(gscv.cv_results_, \"\\n\")\n",
    "print (\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956     procrastinating inevitable drinking problem da...\n",
       "1247    understand concern pain knee surgery pain weig...\n",
       "102     swallow complex act involve mouth throat area ...\n",
       "813        warning eat ask long eat find well way protest\n",
       "994                                          course break\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing Xtest using Spacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])\n",
    "corpus = nlp.pipe(list(Xtest))\n",
    "clean_corpus = [custom_tokenizer(doc) for doc in corpus]\n",
    "Xtest = pd.Series(clean_corpus,index=Xtest.index)\n",
    "Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9454297407912687\n",
      "[[323  35]\n",
      " [  5 370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94       358\n",
      "           1       0.91      0.99      0.95       375\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       733\n",
      "   macro avg       0.95      0.94      0.95       733\n",
      "weighted avg       0.95      0.95      0.95       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict and evaluate best_estimator_ on test data\n",
    "\n",
    "ypred = gscv.best_estimator_.predict(Xtest)\n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(ytest, ypred))\n",
    "print (metrics.confusion_matrix(ytest, ypred))\n",
    "print (metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 323 \n",
      "FP: 35 \n",
      "FN: 5 \n",
      "TP: 370\n"
     ]
    }
   ],
   "source": [
    "#Extract the true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP)\n",
    "\n",
    "TN, FP, FN, TP = metrics.confusion_matrix(y_true=ytest, y_pred=ypred).ravel()\n",
    "print(\"TN:\",TN,\"\\n\"\n",
    "      'FP:',FP,\"\\n\" \n",
    "      'FN:',FN,\"\\n\" \n",
    "      'TP:',TP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9454297407912687"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overall_Accuracy\n",
    "Overall_Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Overall_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Class 0: 0.98\n",
      "Precision for Class 1: 0.91\n"
     ]
    }
   ],
   "source": [
    "#Precision for Class 0 and Class 1\n",
    "\n",
    "Precision_Class_0 = round(TN / (TN + FN),2)\n",
    "print(\"Precision for Class 0:\",Precision_Class_0)\n",
    "\n",
    "Precision_Class_1 = round(TP / (TP + FP),2)\n",
    "print(\"Precision for Class 1:\",Precision_Class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Class 0: 0.9\n",
      "Recall for Class 1: 0.99\n"
     ]
    }
   ],
   "source": [
    "#Recall for Class 0 and Class 1\n",
    "\n",
    "Recall_Class_0 = round(TN / (TN + FP),2)\n",
    "print(\"Recall for Class 0:\",Recall_Class_0)\n",
    "\n",
    "Recall_Class_1 = round(TP / (TP + FN),2)\n",
    "print(\"Recall for Class 1:\",Recall_Class_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for Class 0: 0.94\n",
      "F1-score for Class 1: 0.95\n"
     ]
    }
   ],
   "source": [
    "#F1-Score for Class 0 and Class 1\n",
    "\n",
    "F1score_Class_0 = round((2*Recall_Class_0*Precision_Class_0)/(Recall_Class_0+Precision_Class_0),2)\n",
    "print(\"F1-score for Class 0:\",F1score_Class_0)\n",
    "\n",
    "F1score_Class_1 = round((2*Recall_Class_1*Precision_Class_1)/(Recall_Class_1+Precision_Class_1),2)\n",
    "print(\"F1-score for Class 1:\",F1score_Class_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
